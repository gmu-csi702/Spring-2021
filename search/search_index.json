{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Course Description This course will teach students programming techniques, tools, and debugging methods for using parallel computers to efficiently solve challenging problems in science and engineering. We take an applied approach to learning how to optimize programs serially and then by leveraging some of the most important parallel programming models today: shared memory (OpenMP), distributed memory (MPI), and GPUs (CUDA). By completing a series of mini-projects, students will develop an intuition for identifying the patterns that appear in essentially all programs that need to run fast and learn how to address them using linear algebra, graph algorithms, structured grids, etc. The course culminates in a final project where students code their own parallel program that solves a scientific problem, either from scratch or by modifying an existing code base. Instructor: Dr. Swabir Silayi Classroom: Online (meeting details will be emailed) Meeting times: Wednesdays 7:20pm \u2013 10:00pm Office hours: Online by appointment Platforms Slack workspace for CSI 702 (Spring 2021) - The Slack workspace for this course. XSEDE Course on Moodle - The platform used to distribute the online components of this course. XSEDE Portal - Portal for accessing XSEDE resources and useful tutorials. XSEDE Course on Moodle - The platform used to distribute the online components of this course. CS267 \u2014 Applications of Parallel Computers (Spring 2019) - Course website for the Spring 2019 semester of UC Berkeley\u2019s Applications of Parallel Computers course. The online XSEDE Moodle course is based on this course. Syllabus The full syllabus can be viewed as a pdf here . Software The course lectures and homework assignments are primarily in C and C++. GPUs will be programmed using Nvidia\u2019s CUDA C API and Thrust C++ template library. Software OS Description Slack(standalone app) Windows macOS Linux iOS Android Slack is the primary communication medium for the course, replacing email and serving as a general-purpose discussion board. CLion Windows macOS Linux A cross-platform integrated development environment (IDE) designed specifically for programming in C++. Comes with many useful features enabled and also has a plugin ecosystem. There are introductory tutorial videos available . As a current student, you get a free professional license for the editor if you fill out and submit this form . GitHub Desktop Windows macOS Linux A graphical interface for interacting with GitHub, built by GitHub. The Linux-compatible version is available on GitHub . User documentation from GitHub is available:1- Getting Starting with GitHub Desktop , 2- Contributing to projects with GitHub Desktop Virtualbox Windows macOS Linux VirtualBox is a cross-platform virtualization application that allows you to run multiple OSes, inside multiple virtual machines, at the same time. Intel MKL Windows macOS Linux A library that features highly optimized, threaded, and vectorized math functions that maximize performance on each processor family. It uses industry-standard C and Fortran APIs for compatibility with popular BLAS, LAPACK, and FFTW functions. NVIDIA CUDA Toolkit Windows macOS Linux A toolkit for developing, optimizing and deploying your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers. Includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler and a runtime library to deploy your application. Materials The course has no official textbook, but the following resources will be useful to supplement the course: P. Pacheco, An Introduction to Parallel Programming (Morgan Kaufmann, Amsterdam: Boston, 2011). D. Kirk and W. Hwu, Programming Massively Parallel Processors: A Hands-on Approach, 2nd ed.(Morgan Kaufmann, Amsterdam, 2012). V. Eijkhout, R. van de Geijn, and E. Chow, Introduction to High Performance Scientific Computing (Zenodo, 2016) V. Eijkhout, Introduction to Scientific Programming in C++/Fortran2003 (unpublished) V. Eijkhout, Parallel Programming in MPI and OpenMP (unpublished) License Unless otherwise noted, the course materials in this repository are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .","title":"Home"},{"location":"#course-description","text":"This course will teach students programming techniques, tools, and debugging methods for using parallel computers to efficiently solve challenging problems in science and engineering. We take an applied approach to learning how to optimize programs serially and then by leveraging some of the most important parallel programming models today: shared memory (OpenMP), distributed memory (MPI), and GPUs (CUDA). By completing a series of mini-projects, students will develop an intuition for identifying the patterns that appear in essentially all programs that need to run fast and learn how to address them using linear algebra, graph algorithms, structured grids, etc. The course culminates in a final project where students code their own parallel program that solves a scientific problem, either from scratch or by modifying an existing code base. Instructor: Dr. Swabir Silayi Classroom: Online (meeting details will be emailed) Meeting times: Wednesdays 7:20pm \u2013 10:00pm Office hours: Online by appointment","title":"Course Description"},{"location":"#platforms","text":"Slack workspace for CSI 702 (Spring 2021) - The Slack workspace for this course. XSEDE Course on Moodle - The platform used to distribute the online components of this course. XSEDE Portal - Portal for accessing XSEDE resources and useful tutorials. XSEDE Course on Moodle - The platform used to distribute the online components of this course. CS267 \u2014 Applications of Parallel Computers (Spring 2019) - Course website for the Spring 2019 semester of UC Berkeley\u2019s Applications of Parallel Computers course. The online XSEDE Moodle course is based on this course.","title":"Platforms"},{"location":"#syllabus","text":"The full syllabus can be viewed as a pdf here .","title":"Syllabus"},{"location":"#software","text":"The course lectures and homework assignments are primarily in C and C++. GPUs will be programmed using Nvidia\u2019s CUDA C API and Thrust C++ template library. Software OS Description Slack(standalone app) Windows macOS Linux iOS Android Slack is the primary communication medium for the course, replacing email and serving as a general-purpose discussion board. CLion Windows macOS Linux A cross-platform integrated development environment (IDE) designed specifically for programming in C++. Comes with many useful features enabled and also has a plugin ecosystem. There are introductory tutorial videos available . As a current student, you get a free professional license for the editor if you fill out and submit this form . GitHub Desktop Windows macOS Linux A graphical interface for interacting with GitHub, built by GitHub. The Linux-compatible version is available on GitHub . User documentation from GitHub is available:1- Getting Starting with GitHub Desktop , 2- Contributing to projects with GitHub Desktop Virtualbox Windows macOS Linux VirtualBox is a cross-platform virtualization application that allows you to run multiple OSes, inside multiple virtual machines, at the same time. Intel MKL Windows macOS Linux A library that features highly optimized, threaded, and vectorized math functions that maximize performance on each processor family. It uses industry-standard C and Fortran APIs for compatibility with popular BLAS, LAPACK, and FFTW functions. NVIDIA CUDA Toolkit Windows macOS Linux A toolkit for developing, optimizing and deploying your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers. Includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler and a runtime library to deploy your application.","title":"Software"},{"location":"#materials","text":"The course has no official textbook, but the following resources will be useful to supplement the course: P. Pacheco, An Introduction to Parallel Programming (Morgan Kaufmann, Amsterdam: Boston, 2011). D. Kirk and W. Hwu, Programming Massively Parallel Processors: A Hands-on Approach, 2nd ed.(Morgan Kaufmann, Amsterdam, 2012). V. Eijkhout, R. van de Geijn, and E. Chow, Introduction to High Performance Scientific Computing (Zenodo, 2016) V. Eijkhout, Introduction to Scientific Programming in C++/Fortran2003 (unpublished) V. Eijkhout, Parallel Programming in MPI and OpenMP (unpublished)","title":"Materials"},{"location":"#license","text":"Unless otherwise noted, the course materials in this repository are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License .","title":"License"},{"location":"assignments/","text":"Homeworks Description Due date 0 Describing a Parallel Application Feb-2 @ 11:59pm 1 Optimize Matrix Multiplication Feb-16 @ 11:59pm 2 Parallelizing a particle simulation (Part A) Mar-2 @ 11:59pm 3 Parallelizing a particle simulation (Part B) Mar-16 @ 11:59pm 4 Parallelizing a particle simulation (Part C) Mar-30 @ 11:59pm Final project Description Due date Final project proposal Mar-23 @ 11:59pm Final project report w/ code Apr-28 @ 11:59pm Final project presentation May-05 @ 4:30pm","title":"Assignments"},{"location":"assignments/#homeworks","text":"Description Due date 0 Describing a Parallel Application Feb-2 @ 11:59pm 1 Optimize Matrix Multiplication Feb-16 @ 11:59pm 2 Parallelizing a particle simulation (Part A) Mar-2 @ 11:59pm 3 Parallelizing a particle simulation (Part B) Mar-16 @ 11:59pm 4 Parallelizing a particle simulation (Part C) Mar-30 @ 11:59pm","title":"Homeworks"},{"location":"assignments/#final-project","text":"Description Due date Final project proposal Mar-23 @ 11:59pm Final project report w/ code Apr-28 @ 11:59pm Final project presentation May-05 @ 4:30pm","title":"Final project"},{"location":"schedule/","text":"Course Schedule Week Date Range Lecture Video Topic 1 Jan-27 \u2013 Feb-2 1 Introduction 2 Single Processor Machines 2 Feb-3 \u2013 Feb-9 3 Optimizing Matrix Multiply (cont), Introduction to Data Parallelism 4 Shared-Memory Programming 3 Feb-10 \u2013 Feb-16 5 Roofline and Performance Modeling 6 Sources of Parallelism and Locality in Simulation (Part 1) Video 1 , Video 2 4 Feb-17 \u2013 Feb-23 7 Sources of Parallelism and Locality in Simulation (Part 2) 8 An Introduction to GPGPU Programming 5 Feb-24 \u2013 Mar-2 9 Distributed Memory Machines and Programming 10 Advanced MPI and Collective Communication Algorithms 6 Mar-3 \u2013 Mar-9 11 UPC and UPC++ 12 Cloud Computing and Big Data Processing 7 Mar-10 \u2013 Mar-16 13 Dense Linear Algebra (Part 1) 14 Dense Linear Algebra (Part 2) 8 Mar-17 \u2013 Mar-23 15 Graph Partitioning 16 Automatic Performance Tuning and Sparse-Matrix-Vector-Multiplication (Part 1) 9 Mar-24 \u2013 Mar-30 17 Automatic Performance Tuning and Sparse-Matrix-Vector-Multiplication (Part 2) 18 Structured Grids 10 Mar-31 \u2013 Apr-06 19 Parallel Graph Algorithms 20 Fast Fourier Transform 11 Apr-07 \u2013 Apr-13 21 Sorting and Searching 22 Dynamic Load Balancing 12 Apr-14 \u2013 Apr-28 23 Hierarchical Methods for the N-Body Problem (Part 1) 24 Hierarchical Methods for the N-Body Problem (Part 2) and Communication-Optimal Algorithms and Lower Bounds","title":"Course Schedule"},{"location":"schedule/#course-schedule","text":"Week Date Range Lecture Video Topic 1 Jan-27 \u2013 Feb-2 1 Introduction 2 Single Processor Machines 2 Feb-3 \u2013 Feb-9 3 Optimizing Matrix Multiply (cont), Introduction to Data Parallelism 4 Shared-Memory Programming 3 Feb-10 \u2013 Feb-16 5 Roofline and Performance Modeling 6 Sources of Parallelism and Locality in Simulation (Part 1) Video 1 , Video 2 4 Feb-17 \u2013 Feb-23 7 Sources of Parallelism and Locality in Simulation (Part 2) 8 An Introduction to GPGPU Programming 5 Feb-24 \u2013 Mar-2 9 Distributed Memory Machines and Programming 10 Advanced MPI and Collective Communication Algorithms 6 Mar-3 \u2013 Mar-9 11 UPC and UPC++ 12 Cloud Computing and Big Data Processing 7 Mar-10 \u2013 Mar-16 13 Dense Linear Algebra (Part 1) 14 Dense Linear Algebra (Part 2) 8 Mar-17 \u2013 Mar-23 15 Graph Partitioning 16 Automatic Performance Tuning and Sparse-Matrix-Vector-Multiplication (Part 1) 9 Mar-24 \u2013 Mar-30 17 Automatic Performance Tuning and Sparse-Matrix-Vector-Multiplication (Part 2) 18 Structured Grids 10 Mar-31 \u2013 Apr-06 19 Parallel Graph Algorithms 20 Fast Fourier Transform 11 Apr-07 \u2013 Apr-13 21 Sorting and Searching 22 Dynamic Load Balancing 12 Apr-14 \u2013 Apr-28 23 Hierarchical Methods for the N-Body Problem (Part 1) 24 Hierarchical Methods for the N-Body Problem (Part 2) and Communication-Optimal Algorithms and Lower Bounds","title":"Course Schedule"},{"location":"homework/homework0/","text":"Homework 0 \u2014 Describe a Parallel Application Due: February 1, 2019 @ 11:59pm Instructions First, include a brief bio of you, your research interests, and what you\u2019d like to get out of the class. This will help us form initial assignment groups, and later this will help you find project partners. Examine an application problem for which parallel computing has been used. You may pick a problem from your own research area, somewhere on the web, or elsewhere (so long as it is verifiable). Build a web page briefly describing the application, the use of parallelism, and a frank assessment of its success, weaknesses, and challenges. Some specific details to consider include the following: What is the scientific or engineering problem being solved? How well did the application achieve its scientific / engineering objective? Are simulation results compared to physical results? What parallel platform has the application targeted? (distributed vs. shared memory, vector, etc.) What tools were used to build the application? (languages, libraries, etc.) If the application is run on a major supercomputer, where does that computer rank on the Top 500 list? How well did the application perform? How does this compare to the platform\u2019s best possible performance? Does the application \u201cscale\u201d to large problems on many processors? If you believe it has not, what bottlenecks may have limited its performance? Not all of these details will be available for all applications. You ought to explain what you find noteworthy about the application or its implementation. The write-up should be 1 \u2013 2 pages in length, single-spaced (if you include a figure, then a third is okay). The document\u2019s structure and formatting should be formal and any references you use should be cited and listed in a References section at the bottom of the document. Pick a scientific citation style (I don\u2019t care which one, but pick one) and be consistent with it. Resources Besides Google , you can look at the results from previous classes (at UC Berkeley) for inspiration: Spring 2018 Spring 2017 Spring 2016 Spring 2015 Spring 2013 How to submit Export your write-up to PDF format and upload it to the following two locations: Homework 0 assignment listing on Blackboard Homework 0 assignment listing on Moodle: https://moodle.xsede.org/mod/page/view.php?id=40","title":"Homework0"},{"location":"homework/homework0/#homework-0-describe-a-parallel-application","text":"Due: February 1, 2019 @ 11:59pm","title":"Homework 0 \u2014 Describe a Parallel Application"},{"location":"homework/homework0/#instructions","text":"First, include a brief bio of you, your research interests, and what you\u2019d like to get out of the class. This will help us form initial assignment groups, and later this will help you find project partners. Examine an application problem for which parallel computing has been used. You may pick a problem from your own research area, somewhere on the web, or elsewhere (so long as it is verifiable). Build a web page briefly describing the application, the use of parallelism, and a frank assessment of its success, weaknesses, and challenges. Some specific details to consider include the following: What is the scientific or engineering problem being solved? How well did the application achieve its scientific / engineering objective? Are simulation results compared to physical results? What parallel platform has the application targeted? (distributed vs. shared memory, vector, etc.) What tools were used to build the application? (languages, libraries, etc.) If the application is run on a major supercomputer, where does that computer rank on the Top 500 list? How well did the application perform? How does this compare to the platform\u2019s best possible performance? Does the application \u201cscale\u201d to large problems on many processors? If you believe it has not, what bottlenecks may have limited its performance? Not all of these details will be available for all applications. You ought to explain what you find noteworthy about the application or its implementation. The write-up should be 1 \u2013 2 pages in length, single-spaced (if you include a figure, then a third is okay). The document\u2019s structure and formatting should be formal and any references you use should be cited and listed in a References section at the bottom of the document. Pick a scientific citation style (I don\u2019t care which one, but pick one) and be consistent with it.","title":"Instructions"},{"location":"homework/homework0/#resources","text":"Besides Google , you can look at the results from previous classes (at UC Berkeley) for inspiration: Spring 2018 Spring 2017 Spring 2016 Spring 2015 Spring 2013","title":"Resources"},{"location":"homework/homework0/#how-to-submit","text":"Export your write-up to PDF format and upload it to the following two locations: Homework 0 assignment listing on Blackboard Homework 0 assignment listing on Moodle: https://moodle.xsede.org/mod/page/view.php?id=40","title":"How to submit"},{"location":"homework/homework1/","text":"Homework 1 \u2014 Optimize Matrix Multiplication Due: February 23, 2019 @ 11:59pm GitHub classroom repository: click here to get your starter files The code in this repository is a C++11 refactoring of the code distributed on the Moodle site. If you have only worked with C before, you will find that the most notable change is the lack of `malloc()` and `free()` functions; instead we use the C++11 STL object `std::vector ` to allocate and manage memory for the matrices. `std::vector` implements a kind of \u201cgarbage collection\u201d and frees its memory once it goes out of scope. Problem statement Your task in this assignment is to write an optimized matrix multiplication function for XSEDE\u2019s Bridges supercomputer. We will give you a generic matrix multiplication code (also called matmul or dgemm), and it will be your job to tune our code to run efficiently on Bridge\u2019s processors. We are asking you to write an optimized single-threaded matrix multiply kernel. This will run on only one core. We consider a special case of matmul: C := C + A * B where A , B , and C are n \u2a09 n matrices. This can be performed using 2 n 3 floating point operations ( n 3 adds, n 3 multiplies), as in the following pseudocode: for i = 1 to n for j = 1 to n for k = 1 to n C(i,j) = C(i,j) + A(i,k) * B(k,j) end end end Other things to keep in mind are: The target processor on the Bridges compute nodes is a Xeon Intel 14-Core 64-bit E5-processor running at 2.3GHz and supporting 8 floating-point operations per clock period with a peak performance of 42.9 GFLOPS/core. Besides compiler intrinsic functions and built-ins, your code ( dgemm-blocked.cpp ) must only call into the C/C++ standard library. Your code must use double-precision to represent real numbers. A common reference implementation for Your code should be based on the 2 n 3 algorithm. Submissions that implement algorithms outside this restriction, such as the Strassen algorithm, will not be graded. You may not use compiler flags that automatically detect dgemm kernels and replace them with BLAS calls, i.e. Intel\u2019s -matmul flag. The matrices are all stored in column-major order , i.e. C i,j == C(i,j) == C[(i - 1) + (j - 1) * n] , for i = 1:n , where n is the number of rows in C . Note that we use 1-based indexing when using mathematical symbols C i,j and MATLAB index notation C(i,j) , and 0-based indexing when using C index notation C[(i - 1) + (j - 1) * n] . We will check correctness by the following componentwise error bound, \uff5csquare_dgemm(n, A, B, 0) - A * B\uff5c < \u03b5 * n * \uff5cA\uff5c * \uff5cB\uff5c where \u03b5 = 2 -52 = 2.2 \u2a09 10 -16 is the machine epsilon . Optimizing Now, it\u2019s time to optimize! A few optimizations you might consider adding: Perform blocking. The dgemm-blocked.cpp already gets you started with this, although you\u2019ll need to tune block sizes. Write a register-blocked kernel, either by writing an inner-level fixed-size matrix multiply and hoping (and maybe checking) that the compiler inlines it, writing AVX intrinsics, or even writing inline assembly instructions. Add manual prefetching. You may, of course, proceed however you wish. We recommend you look through the lecture notes as reference material to guide your optimization process, as well as the references at the bottom of this write-up. Report In addition to your code, you are to submit an accompanying report for the homework assignment. The report is to be written in a style appropriate for an academic journal with any relevant citations provided in a bibliography. The report should only discuss the details of the final version of your submitted code. The report is to contain the following: An introduction to the problem A section detailing how you optimized the code. This requires both a conceptual discussion of what you implemented (figures are helpful!), and a code summary, where you explain each important step in your solution (this should be supported with code snippets). A benchmark section where you show how your code performs relative to the naive and blas implementations. Figures are very helpful when reporting this data! Benchmark data should be taken from runs performed on Bridges, not your local computer or laptop. Make sure you discuss the reason for any odd behavior (e.g., dips) in the reported performance. Your report should be converted to the PDF format prior to submission. The filename should follow this format: FirstNameLastName_hw1.pdf . How to submit Put your report file FirstNameLastName_hw1.pdf into the doc/ directory in your GitHub repository, and save, commit, and push the report and the final version of your code into the master branch. Do not push the benchmark binary or any temporary files (such as .o files) to GitHub (the .gitignore file is meant to help prevent this). After everything is pushed and up-to-date, do the following: Upload your report PDF to the Homework 1 assignment posted on Blackboard. Navigate to your repository on the GitHub website, click the green Clone or download button, and click Download ZIP . Rename the zipfile to FirstNameLastName_hw1.zip (replace FirstNameLastName with your first and last name) and upload it to the Moodle site. Logging in to Bridges The easiest way to access the machines is to login directly with your own ssh client to login.xsede.org and from there gsissh into the correct machine. You need to set up two-factor authentication with Duo in order to use the single sign-on hub. More information on is available here on the single login system . An example of logging on to XSEDE would be to first connect to the single sign-on hub: ssh XSEDE-username@login.xsede.org Enter your password and complete the 2-factor authentication request. Then, run the following to hop over to Bridges: gsissh bridges Another way to login to XSEDE resources is via the Accounts tab in XSEDE User Portal. To reach this page login to XSEDE User Portal, navigate to MY XSEDE tab and from there select the Accounts page. All machines you have access to will have a login option next to them that will launch OpenSSH via a Java Applet. Please be aware that most XSEDE resources are not available via direct ssh and all requests must go through the XSEDE single login system. To clone the files from your Github copy, use the following command: git clone git@github.com:mason-sp19-csi-702-003/homework-1-YOURUSERNAME.git Compiling the code Instead of a regular Makefile, compilation is handled via CMake. For your convenience, a simple script file named make.sh is included that will automatically compile your code on Bridges. From the root directory of this repository, simply run: ./make.sh Your code should compile, and the compiled binary will be placed into a folder called bin/ . Running this after you make a change to the code will recompile the changed file and update the binary. If you are developing on your local computer using the virtual machine, you can compile the code by running: ./make.sh vm If you aren\u2019t using the virtual machine, then the wrapper script might not work and you\u2019ll need to run CMake manually. If you are unfamiliar with how to use CMake, you can open make.sh with your text editor to get an idea of how to use it. Submitting and running the code The jobs queue on Bridges is managed via the SLURM scheduler . To submit a job, use the sbatch command like so: sbatch job-blocked To check the status of your running jobs you can use the following command: squeue -u $USER Append a -l flag will print additional information about the running jobs. If you want even more information, consider using the sacct command, for example: sacct -j $JOBID --format JobID,ReqMem,MaxRSS,TotalCPU,State where $JOBID is the ID number of the job. If you want to cancel a job, run: scancel $JOBID If you would like to receive emails for job submissions add the following lines to the submission scripts. This sometimes helps tracking down issues. #SBATCH --mail-type=ALL #SBATCH --mail-user=youremailaddress For more details on SLURM commands please see Bridges documentation page . Finally, there is an interactive mode that allows you to request computing nodes, but maintain a command line. This is ideal for prototyping and debugging purposes. To activate this mode, type interact -N 1 The interact.sh script included in the repository provides you with a reminder on how to activate the interactive session. For additional information, read the documentation on interactive sessions . CLI interface When testing the code on a local computer or within an interactive session on Bridges, you will use a simple command-line interface to select which benchmark you want to run. There are three available benchmark modes: \u201cnaive\u201d, \u201cblocked\u201d, and \u201cblas\u201d. The \u201cblocked\u201d mode is the default. To benchmark the \u201cnaive\u201d mode, you would type: ./benchmark naive To benchmark the \u201cblas\u201d mode, you would type: ./benchmark blas The benchmark results will both print to stdout and be written to a CSV file named benchmark_MODE.csv by default (replace MODE with the benchmarking mode). If you want to change the name of the output file, use the -o flag, for example: benchmark -o benchmark1.csv blocked To read about other features you can set on the command line, such as changing the \u201cmax speed\u201d variable (for example, if you want to benchmark on your own computer), run: ./benchmark --help File transfer When copying files to and from Bridges, you can use scp in conjunction with data.bridges.psc.edu to avoid having to copy your files to Single Site Login node first. This will work with the Two-Factor Authentication setup. Try running the following to copy files directly to Bridges: scp -P 2222 myfile XSEDE-username@data.bridges.psc.edu:/path/to/file To copy from Bridges: scp -P 2222 XSEDE-username@data.bridges.psc.edu:/path/to/file myfile Hints Please carefully read this section for implementation details and hints. Stay tuned to Moodle for updates and clarifications, as well as discussion. If you are new to optimizing numerical codes, we recommend reading the papers in the references section. The GNU and Intel compilers can let you take advantage of Bridges\u2019 Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz processors\u2019 support for AVX2, AVX, SSE4.2, SSE4.1, SSSE3, SSE2, and SSE intrinsic instructions. Click here for a broad overview of what these various definitions mean . To get good performance, you will need to manually vectorize your code using compiler intrinsics. Intel provides a convenient applet containing an interactive, searchable list of all of the available intrinsics functions. One possible optimization to consider for the multiple tuning parameters in an optimized Matrix Multiplication code is autotuning in order to find the optimal/best available value. Libraries like OSKI and ATLAS have shown that achieving the best performance sometimes can be done most efficiently by automatic searching over the parameter space. Some papers on this topic can be found on the Be rkeley B enchmarking and Op timization (BeBOP) page Other notes You may use any compiler available on Bridges. We recommend starting with the GNU C++ compiler (g++), which is what the make.sh script uses automatically. If you use a compiler other than gcc, you will have to change src/CMakeLists.txt , which uses gcc-specific flags. You can type \u201cmodule list\u201d to see which compiler wrapper you have loaded. Note that you may have to switch compilers using the module system. Double-precision matrix multiplication is the dgemm ( d ouble-precision ge neral m atrix- m atrix multiply) routine in the level-3 BLAS . We will compare your implementation with the tuned dgemm implementation available on Bridges, where we will compare with the Intel MKL implementation of dgemm . Note that dgemm has a more general interface than square_dgemm::blocked . An optional part of HW1 encourages you to explore this richer tuning space. Once you have finished and are happy with your square_dgemm::blocked implementation you should consider this and other optional improvements for further coding practice but they will not be graded for HW1. You may make use of C++11 features, such as the STL, if you find it useful, provided they do not implement optimal BLAS routines. Grading Your final code grade will be determined by comparing your code\u2019s sustained performance relative to the Intel MKL implementation (i.e. the \u201cblas\u201d running mode). A quicker estimate of your code grade is outputted at the conclusion of each benchmarking run. It is computed by comparing your code\u2019s sustained performance on Bridges as a percentage of the theoretical peak performance. From that estimate, you will receive a score as follows: If your sustained performance is between 0% and 25% you will receive a score between 0 and 65 proportional to your sustained performance (Ex: 20% gives a score of 52) If your sustained performance is between 25% and 40% you will receive a score between 65 and 80 proportional to your sustained performance (Ex: 35% gives a score of 75) If your sustained performance is between 40% and 70% you will receive a score between 80 and 100 proportional to your sustained performance (Ex: 60% gives a score of 93) If your sustained performance is above 70% you will receive 100 VERY IMPORTANT! Your submission must pass the error bound test and cannot call BLAS for dgemm; any submissions that fail these tests this will receive a grade of 0. For this reason, do not change anything in the benchmark.cpp file, otherwise you might deactivate this check and not realize your code is not behaving properly. Optional These parts are not graded. You should be satisfied with your square_dgemm::blocked results before beginning an optional part. Implement Strassen matmul. Consider switching over to the three-nested-loops algorithm when the recursive subproblems are small enough. Support the dgemm interface (ie, rectangular matrices, transposing, scalar multiples). Try float (single-precision). This means you can use 8-way SIMD parallelism on Bridges. Try complex numbers (single- and double-precision). This forum thread gives advice on vectorizing complex multiplication with the conventional approach, but note that there are other algorithms for this operation. Optimize your matmul for the case when the inputs are symmetric. Consider conventional and packed symmetric storage. Run the optimized code on one of the other supercomputers available and check relative performance and what optimizations need to change or become more relevant References Here are some references that you can use to get started. The PDFs are included in the references/ folder for your convenience. Goto, K., and van de Geijn, R. A. 2008. Anatomy of High-Performance Matrix Multiplication , ACM Transactions on Mathematical Software 34 , 3, Article 12. Note: explains the design decisions for the GotoBLAS dgemm implementation, which also apply to your code. Chellappa, S., Franchetti, F., and P\u00fcschel, M. 2008. How To Write Fast Numerical Code: A Small Introduction , Lecture Notes in Computer Science 5235 , 196-259. Note: how to write C code for modern compilers and memory hierarchies, so that it runs fast. Recommended reading, especially for newcomers to code optimization. Bilmes, et. al. The PHiPAC (Portable High Performance ANSI C) Page for BLAS3 Compatible Fast Matrix Matrix Multiply . Note: PHiPAC is a code-generating autotuner for matmul that started as a submission for this HW in a previous semester of CS267. Also see ATLAS ; both are good examples if you are considering code generation strategies. Lam, M. S., Rothberg, E. E, and Wolf, M. E. 1991. The Cache Performance and Optimization of Blocked Algorithms , ASPLOS\u201991 , 63-74. Note: clearly explains cache blocking, supported by with performance models. Notes on vectorizing with SSE intrinsics and a short video by Brian Van Straalen. Please ignore the due date on the last slide Documentation: Bridges computing environment documentation Intel Xeon Processor E5-2695 v3, 2.3GHz, 35M Cache documentation. You are also welcome to learn from the source code of state-of-art BLAS implementations such as GotoBLAS . However, you should not reuse those codes in your submission.","title":"Homework1"},{"location":"homework/homework1/#homework-1-optimize-matrix-multiplication","text":"Due: February 23, 2019 @ 11:59pm GitHub classroom repository: click here to get your starter files The code in this repository is a C++11 refactoring of the code distributed on the Moodle site. If you have only worked with C before, you will find that the most notable change is the lack of `malloc()` and `free()` functions; instead we use the C++11 STL object `std::vector ` to allocate and manage memory for the matrices. `std::vector` implements a kind of \u201cgarbage collection\u201d and frees its memory once it goes out of scope.","title":"Homework 1 \u2014 Optimize Matrix Multiplication"},{"location":"homework/homework1/#problem-statement","text":"Your task in this assignment is to write an optimized matrix multiplication function for XSEDE\u2019s Bridges supercomputer. We will give you a generic matrix multiplication code (also called matmul or dgemm), and it will be your job to tune our code to run efficiently on Bridge\u2019s processors. We are asking you to write an optimized single-threaded matrix multiply kernel. This will run on only one core. We consider a special case of matmul: C := C + A * B where A , B , and C are n \u2a09 n matrices. This can be performed using 2 n 3 floating point operations ( n 3 adds, n 3 multiplies), as in the following pseudocode: for i = 1 to n for j = 1 to n for k = 1 to n C(i,j) = C(i,j) + A(i,k) * B(k,j) end end end Other things to keep in mind are: The target processor on the Bridges compute nodes is a Xeon Intel 14-Core 64-bit E5-processor running at 2.3GHz and supporting 8 floating-point operations per clock period with a peak performance of 42.9 GFLOPS/core. Besides compiler intrinsic functions and built-ins, your code ( dgemm-blocked.cpp ) must only call into the C/C++ standard library. Your code must use double-precision to represent real numbers. A common reference implementation for Your code should be based on the 2 n 3 algorithm. Submissions that implement algorithms outside this restriction, such as the Strassen algorithm, will not be graded. You may not use compiler flags that automatically detect dgemm kernels and replace them with BLAS calls, i.e. Intel\u2019s -matmul flag. The matrices are all stored in column-major order , i.e. C i,j == C(i,j) == C[(i - 1) + (j - 1) * n] , for i = 1:n , where n is the number of rows in C . Note that we use 1-based indexing when using mathematical symbols C i,j and MATLAB index notation C(i,j) , and 0-based indexing when using C index notation C[(i - 1) + (j - 1) * n] . We will check correctness by the following componentwise error bound, \uff5csquare_dgemm(n, A, B, 0) - A * B\uff5c < \u03b5 * n * \uff5cA\uff5c * \uff5cB\uff5c where \u03b5 = 2 -52 = 2.2 \u2a09 10 -16 is the machine epsilon .","title":"Problem statement"},{"location":"homework/homework1/#optimizing","text":"Now, it\u2019s time to optimize! A few optimizations you might consider adding: Perform blocking. The dgemm-blocked.cpp already gets you started with this, although you\u2019ll need to tune block sizes. Write a register-blocked kernel, either by writing an inner-level fixed-size matrix multiply and hoping (and maybe checking) that the compiler inlines it, writing AVX intrinsics, or even writing inline assembly instructions. Add manual prefetching. You may, of course, proceed however you wish. We recommend you look through the lecture notes as reference material to guide your optimization process, as well as the references at the bottom of this write-up.","title":"Optimizing"},{"location":"homework/homework1/#report","text":"In addition to your code, you are to submit an accompanying report for the homework assignment. The report is to be written in a style appropriate for an academic journal with any relevant citations provided in a bibliography. The report should only discuss the details of the final version of your submitted code. The report is to contain the following: An introduction to the problem A section detailing how you optimized the code. This requires both a conceptual discussion of what you implemented (figures are helpful!), and a code summary, where you explain each important step in your solution (this should be supported with code snippets). A benchmark section where you show how your code performs relative to the naive and blas implementations. Figures are very helpful when reporting this data! Benchmark data should be taken from runs performed on Bridges, not your local computer or laptop. Make sure you discuss the reason for any odd behavior (e.g., dips) in the reported performance. Your report should be converted to the PDF format prior to submission. The filename should follow this format: FirstNameLastName_hw1.pdf .","title":"Report"},{"location":"homework/homework1/#how-to-submit","text":"Put your report file FirstNameLastName_hw1.pdf into the doc/ directory in your GitHub repository, and save, commit, and push the report and the final version of your code into the master branch. Do not push the benchmark binary or any temporary files (such as .o files) to GitHub (the .gitignore file is meant to help prevent this). After everything is pushed and up-to-date, do the following: Upload your report PDF to the Homework 1 assignment posted on Blackboard. Navigate to your repository on the GitHub website, click the green Clone or download button, and click Download ZIP . Rename the zipfile to FirstNameLastName_hw1.zip (replace FirstNameLastName with your first and last name) and upload it to the Moodle site.","title":"How to submit"},{"location":"homework/homework1/#logging-in-to-bridges","text":"The easiest way to access the machines is to login directly with your own ssh client to login.xsede.org and from there gsissh into the correct machine. You need to set up two-factor authentication with Duo in order to use the single sign-on hub. More information on is available here on the single login system . An example of logging on to XSEDE would be to first connect to the single sign-on hub: ssh XSEDE-username@login.xsede.org Enter your password and complete the 2-factor authentication request. Then, run the following to hop over to Bridges: gsissh bridges Another way to login to XSEDE resources is via the Accounts tab in XSEDE User Portal. To reach this page login to XSEDE User Portal, navigate to MY XSEDE tab and from there select the Accounts page. All machines you have access to will have a login option next to them that will launch OpenSSH via a Java Applet. Please be aware that most XSEDE resources are not available via direct ssh and all requests must go through the XSEDE single login system. To clone the files from your Github copy, use the following command: git clone git@github.com:mason-sp19-csi-702-003/homework-1-YOURUSERNAME.git","title":"Logging in to Bridges"},{"location":"homework/homework1/#compiling-the-code","text":"Instead of a regular Makefile, compilation is handled via CMake. For your convenience, a simple script file named make.sh is included that will automatically compile your code on Bridges. From the root directory of this repository, simply run: ./make.sh Your code should compile, and the compiled binary will be placed into a folder called bin/ . Running this after you make a change to the code will recompile the changed file and update the binary. If you are developing on your local computer using the virtual machine, you can compile the code by running: ./make.sh vm If you aren\u2019t using the virtual machine, then the wrapper script might not work and you\u2019ll need to run CMake manually. If you are unfamiliar with how to use CMake, you can open make.sh with your text editor to get an idea of how to use it.","title":"Compiling the code"},{"location":"homework/homework1/#submitting-and-running-the-code","text":"The jobs queue on Bridges is managed via the SLURM scheduler . To submit a job, use the sbatch command like so: sbatch job-blocked To check the status of your running jobs you can use the following command: squeue -u $USER Append a -l flag will print additional information about the running jobs. If you want even more information, consider using the sacct command, for example: sacct -j $JOBID --format JobID,ReqMem,MaxRSS,TotalCPU,State where $JOBID is the ID number of the job. If you want to cancel a job, run: scancel $JOBID If you would like to receive emails for job submissions add the following lines to the submission scripts. This sometimes helps tracking down issues. #SBATCH --mail-type=ALL #SBATCH --mail-user=youremailaddress For more details on SLURM commands please see Bridges documentation page . Finally, there is an interactive mode that allows you to request computing nodes, but maintain a command line. This is ideal for prototyping and debugging purposes. To activate this mode, type interact -N 1 The interact.sh script included in the repository provides you with a reminder on how to activate the interactive session. For additional information, read the documentation on interactive sessions .","title":"Submitting and running the code"},{"location":"homework/homework1/#cli-interface","text":"When testing the code on a local computer or within an interactive session on Bridges, you will use a simple command-line interface to select which benchmark you want to run. There are three available benchmark modes: \u201cnaive\u201d, \u201cblocked\u201d, and \u201cblas\u201d. The \u201cblocked\u201d mode is the default. To benchmark the \u201cnaive\u201d mode, you would type: ./benchmark naive To benchmark the \u201cblas\u201d mode, you would type: ./benchmark blas The benchmark results will both print to stdout and be written to a CSV file named benchmark_MODE.csv by default (replace MODE with the benchmarking mode). If you want to change the name of the output file, use the -o flag, for example: benchmark -o benchmark1.csv blocked To read about other features you can set on the command line, such as changing the \u201cmax speed\u201d variable (for example, if you want to benchmark on your own computer), run: ./benchmark --help","title":"CLI interface"},{"location":"homework/homework1/#file-transfer","text":"When copying files to and from Bridges, you can use scp in conjunction with data.bridges.psc.edu to avoid having to copy your files to Single Site Login node first. This will work with the Two-Factor Authentication setup. Try running the following to copy files directly to Bridges: scp -P 2222 myfile XSEDE-username@data.bridges.psc.edu:/path/to/file To copy from Bridges: scp -P 2222 XSEDE-username@data.bridges.psc.edu:/path/to/file myfile","title":"File transfer"},{"location":"homework/homework1/#hints","text":"Please carefully read this section for implementation details and hints. Stay tuned to Moodle for updates and clarifications, as well as discussion. If you are new to optimizing numerical codes, we recommend reading the papers in the references section. The GNU and Intel compilers can let you take advantage of Bridges\u2019 Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz processors\u2019 support for AVX2, AVX, SSE4.2, SSE4.1, SSSE3, SSE2, and SSE intrinsic instructions. Click here for a broad overview of what these various definitions mean . To get good performance, you will need to manually vectorize your code using compiler intrinsics. Intel provides a convenient applet containing an interactive, searchable list of all of the available intrinsics functions. One possible optimization to consider for the multiple tuning parameters in an optimized Matrix Multiplication code is autotuning in order to find the optimal/best available value. Libraries like OSKI and ATLAS have shown that achieving the best performance sometimes can be done most efficiently by automatic searching over the parameter space. Some papers on this topic can be found on the Be rkeley B enchmarking and Op timization (BeBOP) page","title":"Hints"},{"location":"homework/homework1/#other-notes","text":"You may use any compiler available on Bridges. We recommend starting with the GNU C++ compiler (g++), which is what the make.sh script uses automatically. If you use a compiler other than gcc, you will have to change src/CMakeLists.txt , which uses gcc-specific flags. You can type \u201cmodule list\u201d to see which compiler wrapper you have loaded. Note that you may have to switch compilers using the module system. Double-precision matrix multiplication is the dgemm ( d ouble-precision ge neral m atrix- m atrix multiply) routine in the level-3 BLAS . We will compare your implementation with the tuned dgemm implementation available on Bridges, where we will compare with the Intel MKL implementation of dgemm . Note that dgemm has a more general interface than square_dgemm::blocked . An optional part of HW1 encourages you to explore this richer tuning space. Once you have finished and are happy with your square_dgemm::blocked implementation you should consider this and other optional improvements for further coding practice but they will not be graded for HW1. You may make use of C++11 features, such as the STL, if you find it useful, provided they do not implement optimal BLAS routines.","title":"Other notes"},{"location":"homework/homework1/#grading","text":"Your final code grade will be determined by comparing your code\u2019s sustained performance relative to the Intel MKL implementation (i.e. the \u201cblas\u201d running mode). A quicker estimate of your code grade is outputted at the conclusion of each benchmarking run. It is computed by comparing your code\u2019s sustained performance on Bridges as a percentage of the theoretical peak performance. From that estimate, you will receive a score as follows: If your sustained performance is between 0% and 25% you will receive a score between 0 and 65 proportional to your sustained performance (Ex: 20% gives a score of 52) If your sustained performance is between 25% and 40% you will receive a score between 65 and 80 proportional to your sustained performance (Ex: 35% gives a score of 75) If your sustained performance is between 40% and 70% you will receive a score between 80 and 100 proportional to your sustained performance (Ex: 60% gives a score of 93) If your sustained performance is above 70% you will receive 100 VERY IMPORTANT! Your submission must pass the error bound test and cannot call BLAS for dgemm; any submissions that fail these tests this will receive a grade of 0. For this reason, do not change anything in the benchmark.cpp file, otherwise you might deactivate this check and not realize your code is not behaving properly.","title":"Grading"},{"location":"homework/homework1/#optional","text":"These parts are not graded. You should be satisfied with your square_dgemm::blocked results before beginning an optional part. Implement Strassen matmul. Consider switching over to the three-nested-loops algorithm when the recursive subproblems are small enough. Support the dgemm interface (ie, rectangular matrices, transposing, scalar multiples). Try float (single-precision). This means you can use 8-way SIMD parallelism on Bridges. Try complex numbers (single- and double-precision). This forum thread gives advice on vectorizing complex multiplication with the conventional approach, but note that there are other algorithms for this operation. Optimize your matmul for the case when the inputs are symmetric. Consider conventional and packed symmetric storage. Run the optimized code on one of the other supercomputers available and check relative performance and what optimizations need to change or become more relevant","title":"Optional"},{"location":"homework/homework1/#references","text":"Here are some references that you can use to get started. The PDFs are included in the references/ folder for your convenience. Goto, K., and van de Geijn, R. A. 2008. Anatomy of High-Performance Matrix Multiplication , ACM Transactions on Mathematical Software 34 , 3, Article 12. Note: explains the design decisions for the GotoBLAS dgemm implementation, which also apply to your code. Chellappa, S., Franchetti, F., and P\u00fcschel, M. 2008. How To Write Fast Numerical Code: A Small Introduction , Lecture Notes in Computer Science 5235 , 196-259. Note: how to write C code for modern compilers and memory hierarchies, so that it runs fast. Recommended reading, especially for newcomers to code optimization. Bilmes, et. al. The PHiPAC (Portable High Performance ANSI C) Page for BLAS3 Compatible Fast Matrix Matrix Multiply . Note: PHiPAC is a code-generating autotuner for matmul that started as a submission for this HW in a previous semester of CS267. Also see ATLAS ; both are good examples if you are considering code generation strategies. Lam, M. S., Rothberg, E. E, and Wolf, M. E. 1991. The Cache Performance and Optimization of Blocked Algorithms , ASPLOS\u201991 , 63-74. Note: clearly explains cache blocking, supported by with performance models. Notes on vectorizing with SSE intrinsics and a short video by Brian Van Straalen. Please ignore the due date on the last slide","title":"References"},{"location":"homework/homework1/#documentation","text":"Bridges computing environment documentation Intel Xeon Processor E5-2695 v3, 2.3GHz, 35M Cache documentation. You are also welcome to learn from the source code of state-of-art BLAS implementations such as GotoBLAS . However, you should not reuse those codes in your submission.","title":"Documentation:"},{"location":"homework/homework2a/","text":"Homework 2 \u2014 Parallelizing a Particle Simulation Due: March 9, 2019 @ 11:59pm Part A: Optimizing the serial code GitHub classroom repository: click here to get your team\u2019s starter fiels Problem statement Your goal is to parallelize on XSEDE\u2019s Bridges supercomputer a toy particle simulator (similar particle simulators are used in mechanics , biology , astronomy , etc.) that reproduces the behaviour shown in the following animation: The range of interaction forces (cutoff) is limited as shown in grey for a selected particle. Density is set sufficiently low so that given n particles, only O(n) interactions are expected. Suppose we have a code that runs in time T = O(n) on a single processor. Then we\u2019d hope to run in time T/p when using p processors. We\u2019d like you to write parallel codes that approach these expectations. Correctness and Performance A simple correctness check which computes the minimal distance between 2 particles during the entire simulation is provided. A correct simulation will have particles stay at greater than 0.4 (of cutoff) with typical values between 0.7-0.8 . A simulation were particles don\u2019t interact correctly will be less than 0.4 (of cutoff) with typical values between 0.01-0.05 . More details as well as an average distance are described in the source file. The code we are providing will do this distance check based on the calls to the interaction function but the full autograder will do the checks based on the outputted txt file. We\u2019d recommend keeping the correctness checker in your code, but depending on performance desires it can be deleted as long as the simulation can still output a correct txt file. The performance of the code is determined by doing multiple runs of the simulation with increasing particle numbers and then running a benchmarking script written in Python on the simulation outputs. This can be done automatically with the auto-* scripts. Important note for Performance: While the scripts we are providing have small numbers of particles (500-1000) to allow for the O(n 2 ) algorithm to finish execution, the final codes will be tested with values 100 times larger (50000-100000) to better see their performance. Grading 25% of your grade will be based on your code\u2019s efficiency, and 75% of your grade will depend on your report. Serial Code performance will be tested via fitting multiple runs of the code to a line on a log/log plot and calculating the slope of that line. This will determine whether your code is attaining the O(n) desired complexity versus the starting O(n 2 ) . With an achieved result of O(n x ) you will receive If x is between 2 and 1.5 you will receive a serial score between 0 and 90 proportional to x . (Ex:1.75 gives a score of 45) If x is between 1.5 and 1.4 you will receive a serial score between 90 and 100 proportional to x . (Ex:1.44 gives a score of 96) If x is below 1.4 you will receive a serial score of 100. Report This homework will be completed in groups, and each group must submit a single joint report. The report is to be written in a style appropriate for an academic journal with any relevant citations provided in a bibliography. The report should only discuss the details of the final version of your submitted code. The report is to contain the following: An introduction to the problem A section detailing how you optimized the code. This requires both a conceptual discussion, for example how you partitioned the space in the problem (figures are helpful!), and a code summary, where you explain each important step in your solution (this should be supported with code snippets). A benchmark section where you show how your code performs against the naive implementation, and how it scales with respect to the appropriate parameters. Figures are helpful when reporting this data! Benchmark data should be taken from runs performed on Bridges, not your local computer or laptop. Make sure you discuss the reason for any odd behavior in the reported performance. Your report should be converted to the PDF format prior to submission. The filename should follow this format: Team#_hw2a.pdf . How to submit Put your report file Team#_hw2a.pdf into the doc/ directory in your GitHub repository, and save, commit, and push the report and the final version of your code into the master branch. Do not push the particles binary or any temporary files (such as .o files) to GitHub (the .gitignore file is meant to help prevent this). After everything is pushed and up-to-date, do the following: Have one group member upload the report PDF to the Homework 2a assignment posted on Blackboard. Have each group member navigate to the group\u2019s repository on the GitHub website, click the green Clone or download button, and click Download ZIP . Rename the zipfile to Team#_hw2a.zip (replace # with your team\u2019s number) and upload it to the Moodle site. Source code You will start with the serial implementation supplied below, which runs in O(n 2 ) time, which is unacceptably inefficient. src/cli.cpp (DO NOT EDIT) and vendor/CLI/CLI11.hpp (DO NOT EDIT) Unified interface for defining and controlling command-line options src/serial.cpp and src/serial.hpp a serial implementation src/common.cpp and src/common.hpp an implementation of common functionality, such as I/O, numerics and timing CMakeLists.txt (DO NOT EDIT) and src/CMakeLists.txt cmake configuration files for compiling your code. Compiler flags and the full list of source files to compile can be edited in src/CMakeLists.txt . job-bridges-serial sample batch files to launch jobs on Bridges. Use sbatch to submit on Bridges. Use these files to check correctness auto-bridges-serial sample batch files to launch autograder jobs on Bridges. Use sbatch to submit on Bridges. Use these files to check performance scripts/hw2-visualize/animate.py A Python script for animating the particle trajectories generated by the simulation. Requires a recent version of Anaconda to be installed. Imagemagick must also be installed to generate animated gif files. ffmpeg must also be installed to render mp4 movie files. Look in job-bridges-serial for an example on how to generate the animations. scripts/hw2-autograder/hw2-autograder.py A Python script for grading the scaling of your code. Requires a recent version of Anaconda to be installed. Look in auto-bridges-serial for an example on how to run the autograder. Logging in to Bridges The easiest way to access the machines is to login directly with your own ssh client to login.xsede.org and from there gsissh into the correct machine. You need to set up two-factor authentication with Duo in order to use the single sign-on hub. More information on is available here on the single login system . An example of logging on to XSEDE would be to first connect to the single sign-on hub: ssh XSEDE-username@login.xsede.org Enter your password and complete the 2-factor authentication request. Then, run the following to hop over to Bridges: gsissh bridges Another way to login to XSEDE resources is via the Accounts tab in XSEDE User Portal. To reach this page login to XSEDE User Portal, navigate to MY XSEDE tab and from there select the Accounts page. All machines you have access to will have a login option next to them that will launch OpenSSH via a Java Applet. Please be aware that most XSEDE resources are not available via direct ssh and all requests must go through the XSEDE single login system. To clone the files from your Github copy, use the following command: git clone git@github.com:mason-sp19-csi-702-003/homework-2a-YOURTEAMNAME.git Compiling the code Instead of a regular Makefile, compilation is handled via CMake. For your convenience, a simple script file named make.sh is included that will automatically compile your code on Bridges. From the root directory of this repository, simply run: ./make.sh Your code should compile, and the compiled binary will be placed into a folder called bin/ . Running this after you make a change to the code will recompile the changed file and update the binary. If you are developing on your local computer using the virtual machine, you can compile the code by running: ./make.sh vm If you aren\u2019t using the virtual machine, then the wrapper script might not work and you\u2019ll need to run CMake manually. If you are unfamiliar with how to use CMake, you can open make.sh with your text editor to get an idea of how to use it. Submitting and running the code The jobs queue on Bridges is managed via the SLURM scheduler . To submit a job, use the sbatch command like so: sbatch job-bridges-serial To check the status of your running jobs you can use the following command: squeue -u $USER Append a -l flag will print additional information about the running jobs. If you want even more information, consider using the sacct command, for example: sacct -j $JOBID --format JobID,ReqMem,MaxRSS,TotalCPU,State where $JOBID is the ID number of the job. If you want to cancel a job, run: scancel $JOBID If you would like to receive emails for job submissions add the following lines to the submission scripts. This sometimes helps tracking down issues. #SBATCH --mail-type=ALL #SBATCH --mail-user=youremailaddress For more details on SLURM commands please see Bridges documentation page . Finally, there is an interactive mode that allows you to request computing nodes, but maintain a command line. This is ideal for prototyping and debugging purposes. To activate this mode, type interact -N 1 The interact.sh script included in the repository provides you with a reminder on how to activate the interactive session. For additional information, read the documentation on interactive sessions . CLI interface When testing the code on a local computer or within an interactive session on Bridges, you will use a simple command-line interface to launch the simulation. For Part A, there is only one runtime mode: \u201cserial\u201d. The \u201cserial\u201d mode is also the default. Running ./bin/particles -h will bring up the help: Usage: ./bin/particles [OPTIONS] [mode] Positionals: mode TEXT in {serial} Particle simulation run modes serial: (default) Serial version of simulation. Options: -h,--help Print this help message and exit -n INT=1000 Set the number of particles -o,--output TEXT Specify the output file name -s,--summary TEXT Specify the summary file name --no Turn off all correctness checks and outputs The most important options are -n , -o , and -s . The -n option lets you control the number of particles in the simulation. The -o option lets you output a history of the particle positions to a file, which can be used to generate an animated gif or mp4 file. The -s option lets you save the amount of time it takes to run a simulation for a given number of particles to a file. If the file exists, then new benchmark results are appended to the end of the file. The summary file will be used to compute your code grade. For example, to run a particle simulation with 2000 particles that outputs the benchmark summary data to a file named serial_summary.txt , you would run: ./bin/particles -n 2000 -s serial_summary.txt File transfer When copying files to and from Bridges, you can use scp in conjunction with data.bridges.psc.edu to avoid having to copy your files to Single Site Login node first. This will work with the Two-Factor Authentication setup. Try running the following to copy files directly to Bridges: scp -P 2222 myfile XSEDE-username@data.bridges.psc.edu:/path/to/file To copy from Bridges: scp -P 2222 XSEDE-username@data.bridges.psc.edu:/path/to/file myfile","title":"Homework2a"},{"location":"homework/homework2a/#homework-2-parallelizing-a-particle-simulation","text":"Due: March 9, 2019 @ 11:59pm Part A: Optimizing the serial code GitHub classroom repository: click here to get your team\u2019s starter fiels","title":"Homework 2 \u2014 Parallelizing a Particle Simulation"},{"location":"homework/homework2a/#problem-statement","text":"Your goal is to parallelize on XSEDE\u2019s Bridges supercomputer a toy particle simulator (similar particle simulators are used in mechanics , biology , astronomy , etc.) that reproduces the behaviour shown in the following animation: The range of interaction forces (cutoff) is limited as shown in grey for a selected particle. Density is set sufficiently low so that given n particles, only O(n) interactions are expected. Suppose we have a code that runs in time T = O(n) on a single processor. Then we\u2019d hope to run in time T/p when using p processors. We\u2019d like you to write parallel codes that approach these expectations.","title":"Problem statement"},{"location":"homework/homework2a/#correctness-and-performance","text":"A simple correctness check which computes the minimal distance between 2 particles during the entire simulation is provided. A correct simulation will have particles stay at greater than 0.4 (of cutoff) with typical values between 0.7-0.8 . A simulation were particles don\u2019t interact correctly will be less than 0.4 (of cutoff) with typical values between 0.01-0.05 . More details as well as an average distance are described in the source file. The code we are providing will do this distance check based on the calls to the interaction function but the full autograder will do the checks based on the outputted txt file. We\u2019d recommend keeping the correctness checker in your code, but depending on performance desires it can be deleted as long as the simulation can still output a correct txt file. The performance of the code is determined by doing multiple runs of the simulation with increasing particle numbers and then running a benchmarking script written in Python on the simulation outputs. This can be done automatically with the auto-* scripts.","title":"Correctness and Performance"},{"location":"homework/homework2a/#important-note-for-performance","text":"While the scripts we are providing have small numbers of particles (500-1000) to allow for the O(n 2 ) algorithm to finish execution, the final codes will be tested with values 100 times larger (50000-100000) to better see their performance.","title":"Important note for Performance:"},{"location":"homework/homework2a/#grading","text":"25% of your grade will be based on your code\u2019s efficiency, and 75% of your grade will depend on your report. Serial Code performance will be tested via fitting multiple runs of the code to a line on a log/log plot and calculating the slope of that line. This will determine whether your code is attaining the O(n) desired complexity versus the starting O(n 2 ) . With an achieved result of O(n x ) you will receive If x is between 2 and 1.5 you will receive a serial score between 0 and 90 proportional to x . (Ex:1.75 gives a score of 45) If x is between 1.5 and 1.4 you will receive a serial score between 90 and 100 proportional to x . (Ex:1.44 gives a score of 96) If x is below 1.4 you will receive a serial score of 100.","title":"Grading"},{"location":"homework/homework2a/#report","text":"This homework will be completed in groups, and each group must submit a single joint report. The report is to be written in a style appropriate for an academic journal with any relevant citations provided in a bibliography. The report should only discuss the details of the final version of your submitted code. The report is to contain the following: An introduction to the problem A section detailing how you optimized the code. This requires both a conceptual discussion, for example how you partitioned the space in the problem (figures are helpful!), and a code summary, where you explain each important step in your solution (this should be supported with code snippets). A benchmark section where you show how your code performs against the naive implementation, and how it scales with respect to the appropriate parameters. Figures are helpful when reporting this data! Benchmark data should be taken from runs performed on Bridges, not your local computer or laptop. Make sure you discuss the reason for any odd behavior in the reported performance. Your report should be converted to the PDF format prior to submission. The filename should follow this format: Team#_hw2a.pdf .","title":"Report"},{"location":"homework/homework2a/#how-to-submit","text":"Put your report file Team#_hw2a.pdf into the doc/ directory in your GitHub repository, and save, commit, and push the report and the final version of your code into the master branch. Do not push the particles binary or any temporary files (such as .o files) to GitHub (the .gitignore file is meant to help prevent this). After everything is pushed and up-to-date, do the following: Have one group member upload the report PDF to the Homework 2a assignment posted on Blackboard. Have each group member navigate to the group\u2019s repository on the GitHub website, click the green Clone or download button, and click Download ZIP . Rename the zipfile to Team#_hw2a.zip (replace # with your team\u2019s number) and upload it to the Moodle site.","title":"How to submit"},{"location":"homework/homework2a/#source-code","text":"You will start with the serial implementation supplied below, which runs in O(n 2 ) time, which is unacceptably inefficient. src/cli.cpp (DO NOT EDIT) and vendor/CLI/CLI11.hpp (DO NOT EDIT) Unified interface for defining and controlling command-line options src/serial.cpp and src/serial.hpp a serial implementation src/common.cpp and src/common.hpp an implementation of common functionality, such as I/O, numerics and timing CMakeLists.txt (DO NOT EDIT) and src/CMakeLists.txt cmake configuration files for compiling your code. Compiler flags and the full list of source files to compile can be edited in src/CMakeLists.txt . job-bridges-serial sample batch files to launch jobs on Bridges. Use sbatch to submit on Bridges. Use these files to check correctness auto-bridges-serial sample batch files to launch autograder jobs on Bridges. Use sbatch to submit on Bridges. Use these files to check performance scripts/hw2-visualize/animate.py A Python script for animating the particle trajectories generated by the simulation. Requires a recent version of Anaconda to be installed. Imagemagick must also be installed to generate animated gif files. ffmpeg must also be installed to render mp4 movie files. Look in job-bridges-serial for an example on how to generate the animations. scripts/hw2-autograder/hw2-autograder.py A Python script for grading the scaling of your code. Requires a recent version of Anaconda to be installed. Look in auto-bridges-serial for an example on how to run the autograder.","title":"Source code"},{"location":"homework/homework2a/#logging-in-to-bridges","text":"The easiest way to access the machines is to login directly with your own ssh client to login.xsede.org and from there gsissh into the correct machine. You need to set up two-factor authentication with Duo in order to use the single sign-on hub. More information on is available here on the single login system . An example of logging on to XSEDE would be to first connect to the single sign-on hub: ssh XSEDE-username@login.xsede.org Enter your password and complete the 2-factor authentication request. Then, run the following to hop over to Bridges: gsissh bridges Another way to login to XSEDE resources is via the Accounts tab in XSEDE User Portal. To reach this page login to XSEDE User Portal, navigate to MY XSEDE tab and from there select the Accounts page. All machines you have access to will have a login option next to them that will launch OpenSSH via a Java Applet. Please be aware that most XSEDE resources are not available via direct ssh and all requests must go through the XSEDE single login system. To clone the files from your Github copy, use the following command: git clone git@github.com:mason-sp19-csi-702-003/homework-2a-YOURTEAMNAME.git","title":"Logging in to Bridges"},{"location":"homework/homework2a/#compiling-the-code","text":"Instead of a regular Makefile, compilation is handled via CMake. For your convenience, a simple script file named make.sh is included that will automatically compile your code on Bridges. From the root directory of this repository, simply run: ./make.sh Your code should compile, and the compiled binary will be placed into a folder called bin/ . Running this after you make a change to the code will recompile the changed file and update the binary. If you are developing on your local computer using the virtual machine, you can compile the code by running: ./make.sh vm If you aren\u2019t using the virtual machine, then the wrapper script might not work and you\u2019ll need to run CMake manually. If you are unfamiliar with how to use CMake, you can open make.sh with your text editor to get an idea of how to use it.","title":"Compiling the code"},{"location":"homework/homework2a/#submitting-and-running-the-code","text":"The jobs queue on Bridges is managed via the SLURM scheduler . To submit a job, use the sbatch command like so: sbatch job-bridges-serial To check the status of your running jobs you can use the following command: squeue -u $USER Append a -l flag will print additional information about the running jobs. If you want even more information, consider using the sacct command, for example: sacct -j $JOBID --format JobID,ReqMem,MaxRSS,TotalCPU,State where $JOBID is the ID number of the job. If you want to cancel a job, run: scancel $JOBID If you would like to receive emails for job submissions add the following lines to the submission scripts. This sometimes helps tracking down issues. #SBATCH --mail-type=ALL #SBATCH --mail-user=youremailaddress For more details on SLURM commands please see Bridges documentation page . Finally, there is an interactive mode that allows you to request computing nodes, but maintain a command line. This is ideal for prototyping and debugging purposes. To activate this mode, type interact -N 1 The interact.sh script included in the repository provides you with a reminder on how to activate the interactive session. For additional information, read the documentation on interactive sessions .","title":"Submitting and running the code"},{"location":"homework/homework2a/#cli-interface","text":"When testing the code on a local computer or within an interactive session on Bridges, you will use a simple command-line interface to launch the simulation. For Part A, there is only one runtime mode: \u201cserial\u201d. The \u201cserial\u201d mode is also the default. Running ./bin/particles -h will bring up the help: Usage: ./bin/particles [OPTIONS] [mode] Positionals: mode TEXT in {serial} Particle simulation run modes serial: (default) Serial version of simulation. Options: -h,--help Print this help message and exit -n INT=1000 Set the number of particles -o,--output TEXT Specify the output file name -s,--summary TEXT Specify the summary file name --no Turn off all correctness checks and outputs The most important options are -n , -o , and -s . The -n option lets you control the number of particles in the simulation. The -o option lets you output a history of the particle positions to a file, which can be used to generate an animated gif or mp4 file. The -s option lets you save the amount of time it takes to run a simulation for a given number of particles to a file. If the file exists, then new benchmark results are appended to the end of the file. The summary file will be used to compute your code grade. For example, to run a particle simulation with 2000 particles that outputs the benchmark summary data to a file named serial_summary.txt , you would run: ./bin/particles -n 2000 -s serial_summary.txt","title":"CLI interface"},{"location":"homework/homework2a/#file-transfer","text":"When copying files to and from Bridges, you can use scp in conjunction with data.bridges.psc.edu to avoid having to copy your files to Single Site Login node first. This will work with the Two-Factor Authentication setup. Try running the following to copy files directly to Bridges: scp -P 2222 myfile XSEDE-username@data.bridges.psc.edu:/path/to/file To copy from Bridges: scp -P 2222 XSEDE-username@data.bridges.psc.edu:/path/to/file myfile","title":"File transfer"},{"location":"homework/homework2c/","text":"Homework 2 \u2014 Parallelizing a Particle Simulation Due: April 28, 2019 @ 11:59pm Part C: GPU GitHub classroom repository: click here to get your team\u2019s starter files Important Notice Unlike the other homework assignments, you might not be able to run this on your laptop or personal computer. You MUST have a NVIDIA GPU, and it must be on this Supported GPUs list . If it\u2019s on the list, then you will also need to install the CUDA Toolkit . If you already have the CUDA Toolkit installed, then verify that it\u2019s version 7 or higher. In addition, the settings in the SLURM submission scripts and the way you launch interactive mode on Bridges is different from the other homeworks. Please review the Submitting and running the code section of this file before attempting to run an interactive GPU session. Problem statement Your goal is to parallelize on XSEDE\u2019s Bridges supercomputer a toy particle simulator (similar particle simulators are used in mechanics , biology , astronomy , etc.) that reproduces the behaviour shown in the following animation: The range of interaction forces (cutoff) is limited as shown in grey for a selected particle. Density is set sufficiently low so that given n particles, only O(n) interactions are expected. Suppose we have a code that runs in time T = O(n) on a single processor. Then we\u2019d hope to run in time T/p when using p processors. We\u2019d like you to write parallel codes that approach these expectations. You will be executing your code on a NVIDIA Tesla K80 GPU , which has a compute capability of 3.7 . Correctness and Performance A simple correctness check which computes the minimal distance between 2 particles during the entire simulation is provided. A correct simulation will have particles stay at greater than 0.4 (of cutoff) with typical values between 0.7-0.8 . A simulation were particles don\u2019t interact correctly will be less than 0.4 (of cutoff) with typical values between 0.01-0.05 . Adding the checks inside the GPU code provides too much of an overhead so an autocorrect executable is provided that checks the output txt file for the values mentioned above. Important note for Performance: While the job-bridges-* scripts we are providing have small numbers of particles (4000) to allow for the O(n 2 ) algorithm to finish execution, the final code will be tested with values in the range of auto-bridges-gpu . Grading 25% of your grade will be based on your code\u2019s efficiency, and 75% of your grade will depend on your report. GPU Scaling will be tested via fitting multiple runs of the code to a line on a log/log plot and calculating the slope of that line. This will determine whether your code is attaining the O(n) desired complexity versus the starting O(n 2 ) . With an achieved result of O(n x ) you will receive: If x is between 1.4 and 1.2 you will receive a scaling score between 0 and 40 proportional to x. (Ex: 1.3 gives a score of 20.0) If x is below 1.2 you will receive a scaling score of 40. GPU speedup will be tested by comparing the runs with serial O(n) code and finding the average over a range of particle sizes. Depending on the average speedup the score will be: If the speedup is between 2 and 4 you will receive a score between 0 and 40 proportional to it (Ex: 3 gives a score of 20) If the speedup is between 4 and 8 you will receive a score between 40 and 60 proportional to it (Ex: 7 gives a score of 55) If the speedup is above 8 you will receive a score of 60 If the scaling score for the GPU is 0 (aka still have O(n 2 ) code ) the score for the speedup will be set to 0. The total code grade is the sum of the GPU scaling and speedup scores. Report This homework will be completed in groups, and each group must submit a single joint report. The report is to be written in a style appropriate for an academic journal with any relevant citations provided in a bibliography. The report should only discuss the details of the final version of your submitted code. The report is to contain the following: A section detailing how you optimized the code. This requires both a conceptual discussion, for example how you partitioned the space in the problem (figures are helpful!), and a code summary, where you explain each important step in your solution (this should be supported with code snippets). A benchmark section where you show how your code performs against the naive implementation, and how it scales with respect to the appropriate parameters. Figures are helpful when reporting this data! Benchmark data should be taken from runs performed on Bridges, not your local computer or laptop. Make sure you discuss the reason for any odd behavior in the reported performance. As you can see, no introductory section is needed for this report as the simulation and general problem is the same as in Part A and Part B. Your report should be converted to the PDF format prior to submission. The filename should follow this format: Team#_hw2c.pdf . How to submit Put your report file Team#_hw2c.pdf into the doc/ directory in your GitHub repository, and save, commit, and push the report and the final version of your code into the master branch. Do not push the particles binary or any temporary files (such as .o files) to GitHub (the .gitignore file is meant to help prevent this). After everything is pushed and up-to-date, do the following: Have one group member upload the report PDF to the Homework 2c assignment posted on Blackboard. Have each group member navigate to the group\u2019s repository on the GitHub website, click the green Clone or download button, and click Download ZIP . Rename the zipfile to Team#_hw2c.zip (replace # with your team\u2019s number) and upload it to the Moodle site. Source code You will start with an OpenMP and MPI implementation that is unacceptably inefficient. You are also provided with an efficient solution for the serial implementation. You are highly encouraged to port this over to your OpenMP and MPI implementations. src/cli.cu (DO NOT EDIT) and vendor/CLI/CLI11.hpp (DO NOT EDIT) Unified interface for defining and controlling command-line options src/serial.cu and src/serial.cuh an efficient serial implementation with O(n) scaling. src/common.cu and src/common.cuh an implementation of common functionality, such as I/O, numerics and timing src/gpu.cu and src/gpu.cuh a serial CUDA implementation, similar to the OpenMP and MPI codes you started from. src/gpu_kernels.cu and src/gpu_kernels.cuh The CUDA kernels (functions) used in gpu.cu . src/clion.cuh CLion doesn\u2019t natively support CUDA syntax, so this header file helps fix that. This header file is ignored during compilation, so it has no effect on performance. src/autocorrect A pre-built binary for checking the correctness of a program by verifying the txt output file. This program will run at a serial O(n) speed and will give the results for the minimum and maximum distance between particles during the run. This program can be run on any txt output file from a simulation with small sized tests ( n \\< 10000 ). CMakeLists.txt (DO NOT EDIT) and src/CMakeLists.txt cmake configuration files for compiling your code. Compiler flags and the full list of source files to compile can be edited in src/CMakeLists.txt . job-bridges-gpu sample batch file to launch job on Bridges. Use sbatch to submit on Bridges. This file runs the src/autocorrect test to check for correctness. auto-bridges-serial sample batch file to launch autograder jobs on Bridges. Use sbatch to submit on Bridges. Use this file to check performance. scripts/hw2-visualize/animate.py A Python script for animating the particle trajectories generated by the simulation. Requires a recent version of Anaconda to be installed. Imagemagick must also be installed to generate animated gif files. ffmpeg must also be installed to render mp4 movie files. Look in job-bridges-serial for an example on how to generate the animations. scripts/hw2-autograder/hw2c_autograder.py A Python script for grading your code. Requires a recent version of Anaconda to be installed. Look in auto-bridges-openmp16 and auto-bridges-gpu for an example on how to run the autograder. scripts/get-cuda-sm/get_cuda_sm.sh A small script that, if it runs successfully on your setup, will print out the compute capability of your GPU. Logging in to Bridges The easiest way to access the machines is to login directly with your own ssh client to login.xsede.org and from there gsissh into the correct machine. You need to set up two-factor authentication with Duo in order to use the single sign-on hub. More information on is available here on the single login system . An example of logging on to XSEDE would be to first connect to the single sign-on hub: ssh XSEDE-username@login.xsede.org Enter your password and complete the 2-factor authentication request. Then, run the following to hop over to Bridges: gsissh bridges Another way to login to XSEDE resources is via the Accounts tab in XSEDE User Portal. To reach this page login to XSEDE User Portal, navigate to MY XSEDE tab and from there select the Accounts page. All machines you have access to will have a login option next to them that will launch OpenSSH via a Java Applet. Please be aware that most XSEDE resources are not available via direct ssh and all requests must go through the XSEDE single login system. To clone the files from your Github copy, use the following command: git clone git@github.com:mason-sp19-csi-702-003/homework-2c-YOURTEAMNAME.git Compiling the code Instead of a regular Makefile, compilation is handled via CMake. For your convenience, a simple script file named make.sh is included that will automatically compile your code on Bridges. From the root directory of this repository, simply run: ./make.sh Your code should compile, and the compiled binary will be placed into a folder called bin/ . Running this after you make a change to the code will recompile the changed file and update the binary. If you are developing on your local computer, you can compile the code by running: ./make.sh nomodule The virtual machine is not configured for this assignment even if you have a CUDA-enabled GPU (I am not sure it\u2019s even possible for CUDA code to run in a VirtualBox environment), so you\u2019ll need to have all the relevant libraries installed and run CMake manually. If you are unfamiliar with how to use CMake, you can open make.sh with your text editor to get an idea of how to use it. Submitting and running the code The jobs queue on Bridges is managed via the SLURM scheduler . To submit a job, use the sbatch command like so: sbatch job-bridges-gpu To check the status of your running jobs you can use the following command: squeue -u $USER Append a -l flag will print additional information about the running jobs. If you want even more information, consider using the sacct command, for example: sacct -j $JOBID --format JobID,ReqMem,MaxRSS,TotalCPU,State where $JOBID is the ID number of the job. If you want to cancel a job, run: scancel $JOBID If you would like to receive emails for job submissions add the following lines to the submission scripts. This sometimes helps tracking down issues. #SBATCH --mail-type=ALL #SBATCH --mail-user=youremailaddress For more details on SLURM commands please see Bridges documentation page . Finally, there is an interactive mode that allows you to request computing nodes, but maintain a command line. This is ideal for prototyping and debugging purposes. To activate this mode, type interact -p GPU-shared -N 1 --gres=gpu:k80:1 --ntasks-per-node=1 The interact.sh script included in the repository provides you with a reminder on how to activate the interactive session. For additional information, read the documentation on interactive sessions . CLI interface When testing the code on a local computer or within an interactive session on Bridges, you will use a simple command-line interface to launch the simulation. For Part C, there are two runtime modes: \u201cserial\u201d and \u201cgpu\u201d. The \u201cserial\u201d mode is the default. Running ./bin/particles -h will bring up the help: Usage: ./bin/particles [OPTIONS] [mode] Positionals: mode TEXT in {gpu,serial} Particle simulation run modes serial: (default) Serial version of simulation. gpu: GPU/CUDA version of simulation. Options: -h,--help Print this help message and exit -n INT=1000 Set the number of particles -o,--output TEXT Specify the output file name -s,--summary TEXT Specify the summary file name The most important options are -n , -o , and -s . The -n option lets you control the number of particles in the simulation. The -o option lets you output a history of the particle positions to a file, which can be used to generate an animated gif or mp4 file. The -s option lets you save the amount of time it takes to run a simulation for a given number of particles to a file. If the file exists, then new benchmark results are appended to the end of the file. The summary file will be used to compute your code grade. For example, to run a particle simulation with 2000 particles that outputs the benchmark summary data to a file named serial_summary.txt , you would run: ./bin/particles -n 2000 -s serial_summary.txt To run the same benchmark on the GPU, you would run: ./bin/particles -n 2000 -s gpu_summary.txt gpu File transfer When copying files to and from Bridges, you can use scp in conjunction with data.bridges.psc.edu to avoid having to copy your files to Single Site Login node first. This will work with the Two-Factor Authentication setup. Try running the following to copy files directly to Bridges: scp -P 2222 myfile XSEDE-username@data.bridges.psc.edu:/path/to/file To copy from Bridges: scp -P 2222 XSEDE-username@data.bridges.psc.edu:/path/to/file myfile Optional: Other improvements As always start on these only if you are happy with your current implementation for the GPU code. Creating a Hybrid that uses CUDA per node and MPI between nodes","title":"Homework2c"},{"location":"homework/homework2c/#homework-2-parallelizing-a-particle-simulation","text":"Due: April 28, 2019 @ 11:59pm Part C: GPU GitHub classroom repository: click here to get your team\u2019s starter files","title":"Homework 2 \u2014 Parallelizing a Particle Simulation"},{"location":"homework/homework2c/#important-notice","text":"Unlike the other homework assignments, you might not be able to run this on your laptop or personal computer. You MUST have a NVIDIA GPU, and it must be on this Supported GPUs list . If it\u2019s on the list, then you will also need to install the CUDA Toolkit . If you already have the CUDA Toolkit installed, then verify that it\u2019s version 7 or higher. In addition, the settings in the SLURM submission scripts and the way you launch interactive mode on Bridges is different from the other homeworks. Please review the Submitting and running the code section of this file before attempting to run an interactive GPU session.","title":"Important Notice"},{"location":"homework/homework2c/#problem-statement","text":"Your goal is to parallelize on XSEDE\u2019s Bridges supercomputer a toy particle simulator (similar particle simulators are used in mechanics , biology , astronomy , etc.) that reproduces the behaviour shown in the following animation: The range of interaction forces (cutoff) is limited as shown in grey for a selected particle. Density is set sufficiently low so that given n particles, only O(n) interactions are expected. Suppose we have a code that runs in time T = O(n) on a single processor. Then we\u2019d hope to run in time T/p when using p processors. We\u2019d like you to write parallel codes that approach these expectations. You will be executing your code on a NVIDIA Tesla K80 GPU , which has a compute capability of 3.7 .","title":"Problem statement"},{"location":"homework/homework2c/#correctness-and-performance","text":"A simple correctness check which computes the minimal distance between 2 particles during the entire simulation is provided. A correct simulation will have particles stay at greater than 0.4 (of cutoff) with typical values between 0.7-0.8 . A simulation were particles don\u2019t interact correctly will be less than 0.4 (of cutoff) with typical values between 0.01-0.05 . Adding the checks inside the GPU code provides too much of an overhead so an autocorrect executable is provided that checks the output txt file for the values mentioned above.","title":"Correctness and Performance"},{"location":"homework/homework2c/#important-note-for-performance","text":"While the job-bridges-* scripts we are providing have small numbers of particles (4000) to allow for the O(n 2 ) algorithm to finish execution, the final code will be tested with values in the range of auto-bridges-gpu .","title":"Important note for Performance:"},{"location":"homework/homework2c/#grading","text":"25% of your grade will be based on your code\u2019s efficiency, and 75% of your grade will depend on your report. GPU Scaling will be tested via fitting multiple runs of the code to a line on a log/log plot and calculating the slope of that line. This will determine whether your code is attaining the O(n) desired complexity versus the starting O(n 2 ) . With an achieved result of O(n x ) you will receive: If x is between 1.4 and 1.2 you will receive a scaling score between 0 and 40 proportional to x. (Ex: 1.3 gives a score of 20.0) If x is below 1.2 you will receive a scaling score of 40. GPU speedup will be tested by comparing the runs with serial O(n) code and finding the average over a range of particle sizes. Depending on the average speedup the score will be: If the speedup is between 2 and 4 you will receive a score between 0 and 40 proportional to it (Ex: 3 gives a score of 20) If the speedup is between 4 and 8 you will receive a score between 40 and 60 proportional to it (Ex: 7 gives a score of 55) If the speedup is above 8 you will receive a score of 60 If the scaling score for the GPU is 0 (aka still have O(n 2 ) code ) the score for the speedup will be set to 0. The total code grade is the sum of the GPU scaling and speedup scores.","title":"Grading"},{"location":"homework/homework2c/#report","text":"This homework will be completed in groups, and each group must submit a single joint report. The report is to be written in a style appropriate for an academic journal with any relevant citations provided in a bibliography. The report should only discuss the details of the final version of your submitted code. The report is to contain the following: A section detailing how you optimized the code. This requires both a conceptual discussion, for example how you partitioned the space in the problem (figures are helpful!), and a code summary, where you explain each important step in your solution (this should be supported with code snippets). A benchmark section where you show how your code performs against the naive implementation, and how it scales with respect to the appropriate parameters. Figures are helpful when reporting this data! Benchmark data should be taken from runs performed on Bridges, not your local computer or laptop. Make sure you discuss the reason for any odd behavior in the reported performance. As you can see, no introductory section is needed for this report as the simulation and general problem is the same as in Part A and Part B. Your report should be converted to the PDF format prior to submission. The filename should follow this format: Team#_hw2c.pdf .","title":"Report"},{"location":"homework/homework2c/#how-to-submit","text":"Put your report file Team#_hw2c.pdf into the doc/ directory in your GitHub repository, and save, commit, and push the report and the final version of your code into the master branch. Do not push the particles binary or any temporary files (such as .o files) to GitHub (the .gitignore file is meant to help prevent this). After everything is pushed and up-to-date, do the following: Have one group member upload the report PDF to the Homework 2c assignment posted on Blackboard. Have each group member navigate to the group\u2019s repository on the GitHub website, click the green Clone or download button, and click Download ZIP . Rename the zipfile to Team#_hw2c.zip (replace # with your team\u2019s number) and upload it to the Moodle site.","title":"How to submit"},{"location":"homework/homework2c/#source-code","text":"You will start with an OpenMP and MPI implementation that is unacceptably inefficient. You are also provided with an efficient solution for the serial implementation. You are highly encouraged to port this over to your OpenMP and MPI implementations. src/cli.cu (DO NOT EDIT) and vendor/CLI/CLI11.hpp (DO NOT EDIT) Unified interface for defining and controlling command-line options src/serial.cu and src/serial.cuh an efficient serial implementation with O(n) scaling. src/common.cu and src/common.cuh an implementation of common functionality, such as I/O, numerics and timing src/gpu.cu and src/gpu.cuh a serial CUDA implementation, similar to the OpenMP and MPI codes you started from. src/gpu_kernels.cu and src/gpu_kernels.cuh The CUDA kernels (functions) used in gpu.cu . src/clion.cuh CLion doesn\u2019t natively support CUDA syntax, so this header file helps fix that. This header file is ignored during compilation, so it has no effect on performance. src/autocorrect A pre-built binary for checking the correctness of a program by verifying the txt output file. This program will run at a serial O(n) speed and will give the results for the minimum and maximum distance between particles during the run. This program can be run on any txt output file from a simulation with small sized tests ( n \\< 10000 ). CMakeLists.txt (DO NOT EDIT) and src/CMakeLists.txt cmake configuration files for compiling your code. Compiler flags and the full list of source files to compile can be edited in src/CMakeLists.txt . job-bridges-gpu sample batch file to launch job on Bridges. Use sbatch to submit on Bridges. This file runs the src/autocorrect test to check for correctness. auto-bridges-serial sample batch file to launch autograder jobs on Bridges. Use sbatch to submit on Bridges. Use this file to check performance. scripts/hw2-visualize/animate.py A Python script for animating the particle trajectories generated by the simulation. Requires a recent version of Anaconda to be installed. Imagemagick must also be installed to generate animated gif files. ffmpeg must also be installed to render mp4 movie files. Look in job-bridges-serial for an example on how to generate the animations. scripts/hw2-autograder/hw2c_autograder.py A Python script for grading your code. Requires a recent version of Anaconda to be installed. Look in auto-bridges-openmp16 and auto-bridges-gpu for an example on how to run the autograder. scripts/get-cuda-sm/get_cuda_sm.sh A small script that, if it runs successfully on your setup, will print out the compute capability of your GPU.","title":"Source code"},{"location":"homework/homework2c/#logging-in-to-bridges","text":"The easiest way to access the machines is to login directly with your own ssh client to login.xsede.org and from there gsissh into the correct machine. You need to set up two-factor authentication with Duo in order to use the single sign-on hub. More information on is available here on the single login system . An example of logging on to XSEDE would be to first connect to the single sign-on hub: ssh XSEDE-username@login.xsede.org Enter your password and complete the 2-factor authentication request. Then, run the following to hop over to Bridges: gsissh bridges Another way to login to XSEDE resources is via the Accounts tab in XSEDE User Portal. To reach this page login to XSEDE User Portal, navigate to MY XSEDE tab and from there select the Accounts page. All machines you have access to will have a login option next to them that will launch OpenSSH via a Java Applet. Please be aware that most XSEDE resources are not available via direct ssh and all requests must go through the XSEDE single login system. To clone the files from your Github copy, use the following command: git clone git@github.com:mason-sp19-csi-702-003/homework-2c-YOURTEAMNAME.git","title":"Logging in to Bridges"},{"location":"homework/homework2c/#compiling-the-code","text":"Instead of a regular Makefile, compilation is handled via CMake. For your convenience, a simple script file named make.sh is included that will automatically compile your code on Bridges. From the root directory of this repository, simply run: ./make.sh Your code should compile, and the compiled binary will be placed into a folder called bin/ . Running this after you make a change to the code will recompile the changed file and update the binary. If you are developing on your local computer, you can compile the code by running: ./make.sh nomodule The virtual machine is not configured for this assignment even if you have a CUDA-enabled GPU (I am not sure it\u2019s even possible for CUDA code to run in a VirtualBox environment), so you\u2019ll need to have all the relevant libraries installed and run CMake manually. If you are unfamiliar with how to use CMake, you can open make.sh with your text editor to get an idea of how to use it.","title":"Compiling the code"},{"location":"homework/homework2c/#submitting-and-running-the-code","text":"The jobs queue on Bridges is managed via the SLURM scheduler . To submit a job, use the sbatch command like so: sbatch job-bridges-gpu To check the status of your running jobs you can use the following command: squeue -u $USER Append a -l flag will print additional information about the running jobs. If you want even more information, consider using the sacct command, for example: sacct -j $JOBID --format JobID,ReqMem,MaxRSS,TotalCPU,State where $JOBID is the ID number of the job. If you want to cancel a job, run: scancel $JOBID If you would like to receive emails for job submissions add the following lines to the submission scripts. This sometimes helps tracking down issues. #SBATCH --mail-type=ALL #SBATCH --mail-user=youremailaddress For more details on SLURM commands please see Bridges documentation page . Finally, there is an interactive mode that allows you to request computing nodes, but maintain a command line. This is ideal for prototyping and debugging purposes. To activate this mode, type interact -p GPU-shared -N 1 --gres=gpu:k80:1 --ntasks-per-node=1 The interact.sh script included in the repository provides you with a reminder on how to activate the interactive session. For additional information, read the documentation on interactive sessions .","title":"Submitting and running the code"},{"location":"homework/homework2c/#cli-interface","text":"When testing the code on a local computer or within an interactive session on Bridges, you will use a simple command-line interface to launch the simulation. For Part C, there are two runtime modes: \u201cserial\u201d and \u201cgpu\u201d. The \u201cserial\u201d mode is the default. Running ./bin/particles -h will bring up the help: Usage: ./bin/particles [OPTIONS] [mode] Positionals: mode TEXT in {gpu,serial} Particle simulation run modes serial: (default) Serial version of simulation. gpu: GPU/CUDA version of simulation. Options: -h,--help Print this help message and exit -n INT=1000 Set the number of particles -o,--output TEXT Specify the output file name -s,--summary TEXT Specify the summary file name The most important options are -n , -o , and -s . The -n option lets you control the number of particles in the simulation. The -o option lets you output a history of the particle positions to a file, which can be used to generate an animated gif or mp4 file. The -s option lets you save the amount of time it takes to run a simulation for a given number of particles to a file. If the file exists, then new benchmark results are appended to the end of the file. The summary file will be used to compute your code grade. For example, to run a particle simulation with 2000 particles that outputs the benchmark summary data to a file named serial_summary.txt , you would run: ./bin/particles -n 2000 -s serial_summary.txt To run the same benchmark on the GPU, you would run: ./bin/particles -n 2000 -s gpu_summary.txt gpu","title":"CLI interface"},{"location":"homework/homework2c/#file-transfer","text":"When copying files to and from Bridges, you can use scp in conjunction with data.bridges.psc.edu to avoid having to copy your files to Single Site Login node first. This will work with the Two-Factor Authentication setup. Try running the following to copy files directly to Bridges: scp -P 2222 myfile XSEDE-username@data.bridges.psc.edu:/path/to/file To copy from Bridges: scp -P 2222 XSEDE-username@data.bridges.psc.edu:/path/to/file myfile","title":"File transfer"},{"location":"homework/homework2c/#optional-other-improvements","text":"As always start on these only if you are happy with your current implementation for the GPU code. Creating a Hybrid that uses CUDA per node and MPI between nodes","title":"Optional: Other improvements"}]}